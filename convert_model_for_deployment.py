#!/usr/bin/env python3
"""
æ¨¡å‹è½¬æ¢å’Œä¼˜åŒ–è„šæœ¬
å°†è®­ç»ƒå¥½çš„æ¨¡å‹è½¬æ¢ä¸ºä¸åŒæ ¼å¼ä»¥ä¾¿éƒ¨ç½²
"""

import torch
import timm
import onnx
import numpy as np
from pathlib import Path
import json
import time

class ModelConverter:
    def __init__(self, model_path, device='cpu'):
        """
        åˆå§‹åŒ–æ¨¡å‹è½¬æ¢å™¨
        
        Args:
            model_path: åŸå§‹æ¨¡å‹æƒé‡è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device
        self.model_path = Path(model_path)
        
        # åŠ è½½åŸå§‹æ¨¡å‹
        self.model = timm.create_model('efficientnet_b0', num_classes=2)
        checkpoint = torch.load(model_path, map_location=device)
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        self.model.load_state_dict(state_dict)
        self.model.eval()
        self.model.to(device)
        
        # è·å–æ•°æ®é…ç½®
        self.data_config = timm.data.resolve_data_config({}, model=self.model)
        self.input_size = self.data_config['input_size']
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"ğŸ“ è¾“å…¥å°ºå¯¸: {self.input_size}")
        print(f"ğŸ“± è®¾å¤‡: {device}")

    def convert_to_torchscript(self, output_path):
        """è½¬æ¢ä¸ºTorchScriptæ ¼å¼"""
        print(f"\nğŸ”„ è½¬æ¢ä¸ºTorchScriptæ ¼å¼...")
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        example_input = torch.randn(1, *self.input_size).to(self.device)
        
        # è½¬æ¢ä¸ºTorchScript
        traced_model = torch.jit.trace(self.model, example_input)
        
        # ä¿å­˜æ¨¡å‹
        output_path = Path(output_path)
        traced_model.save(str(output_path))
        
        # éªŒè¯è½¬æ¢
        loaded_model = torch.jit.load(str(output_path))
        with torch.no_grad():
            original_output = self.model(example_input)
            traced_output = loaded_model(example_input)
            diff = torch.abs(original_output - traced_output).max().item()
        
        print(f"âœ… TorchScriptæ¨¡å‹å·²ä¿å­˜: {output_path}")
        print(f"ğŸ” è¾“å‡ºå·®å¼‚: {diff:.8f}")
        
        return output_path

    def convert_to_onnx(self, output_path):
        """è½¬æ¢ä¸ºONNXæ ¼å¼"""
        print(f"\nğŸ”„ è½¬æ¢ä¸ºONNXæ ¼å¼...")
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        example_input = torch.randn(1, *self.input_size).to(self.device)
        
        # å¯¼å‡ºONNX
        output_path = Path(output_path)
        torch.onnx.export(
            self.model,
            example_input,
            str(output_path),
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        
        # éªŒè¯ONNXæ¨¡å‹
        try:
            onnx_model = onnx.load(str(output_path))
            onnx.checker.check_model(onnx_model)
            print(f"âœ… ONNXæ¨¡å‹å·²ä¿å­˜: {output_path}")
            print(f"ğŸ” ONNXæ¨¡å‹éªŒè¯é€šè¿‡")
        except Exception as e:
            print(f"âŒ ONNXæ¨¡å‹éªŒè¯å¤±è´¥: {e}")
        
        return output_path

    def create_optimized_pytorch_model(self, output_path):
        """åˆ›å»ºä¼˜åŒ–çš„PyTorchæ¨¡å‹"""
        print(f"\nğŸ”„ åˆ›å»ºä¼˜åŒ–çš„PyTorchæ¨¡å‹...")
        
        # æ¨¡å‹ä¼˜åŒ–
        optimized_model = torch.jit.optimize_for_inference(
            torch.jit.script(self.model)
        )
        
        # ä¿å­˜ä¼˜åŒ–æ¨¡å‹
        output_path = Path(output_path)
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'model_name': 'efficientnet_b0',
                'num_classes': 2,
                'input_size': self.input_size,
                'class_names': ['bilei', 'waiguan']
            },
            'data_config': self.data_config
        }, output_path)
        
        print(f"âœ… ä¼˜åŒ–PyTorchæ¨¡å‹å·²ä¿å­˜: {output_path}")
        return output_path

    def benchmark_models(self, models_dict, num_runs=100):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"\nâ±ï¸  æ€§èƒ½åŸºå‡†æµ‹è¯• (è¿è¡Œ {num_runs} æ¬¡)...")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randn(1, *self.input_size).to(self.device)
        
        results = {}
        
        for model_name, model_path in models_dict.items():
            print(f"\næµ‹è¯• {model_name}...")
            
            # åŠ è½½æ¨¡å‹
            if model_name == 'original':
                model = self.model
            elif model_name == 'torchscript':
                model = torch.jit.load(str(model_path))
            else:
                continue  # ONNXéœ€è¦ä¸“é—¨çš„è¿è¡Œæ—¶
            
            # é¢„çƒ­
            with torch.no_grad():
                for _ in range(10):
                    _ = model(test_input)
            
            # è®¡æ—¶æµ‹è¯•
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start_time = time.time()
            
            with torch.no_grad():
                for _ in range(num_runs):
                    output = model(test_input)
            
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()
            
            avg_time = (end_time - start_time) / num_runs * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            results[model_name] = {
                'avg_inference_time_ms': avg_time,
                'throughput_fps': 1000 / avg_time,
                'model_size_mb': Path(model_path).stat().st_size / (1024*1024) if model_path else 'N/A'
            }
            
            print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} ms")
            print(f"   ååé‡: {1000/avg_time:.1f} FPS")
        
        return results

    def create_deployment_package(self, output_dir):
        """åˆ›å»ºå®Œæ•´çš„éƒ¨ç½²åŒ…"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        print(f"\nğŸ“¦ åˆ›å»ºéƒ¨ç½²åŒ…: {output_dir}")
        
        # 1. è½¬æ¢æ¨¡å‹
        models = {}
        
        # TorchScript
        torchscript_path = output_dir / "model_torchscript.pt"
        models['torchscript'] = self.convert_to_torchscript(torchscript_path)
        
        # ONNX
        onnx_path = output_dir / "model.onnx"
        models['onnx'] = self.convert_to_onnx(onnx_path)
        
        # ä¼˜åŒ–PyTorch
        optimized_path = output_dir / "model_optimized.pth"
        models['optimized'] = self.create_optimized_pytorch_model(optimized_path)
        
        # 2. æ€§èƒ½æµ‹è¯•
        benchmark_results = self.benchmark_models({
            'original': None,
            'torchscript': torchscript_path
        })
        
        # 3. åˆ›å»ºé…ç½®æ–‡ä»¶
        config = {
            'model_info': {
                'architecture': 'efficientnet_b0',
                'num_classes': 2,
                'class_names': ['bilei', 'waiguan'],
                'input_size': self.input_size,
                'data_config': self.data_config
            },
            'available_formats': {
                'torchscript': str(torchscript_path.name),
                'onnx': str(onnx_path.name),
                'optimized_pytorch': str(optimized_path.name)
            },
            'benchmark_results': benchmark_results,
            'usage_examples': {
                'torchscript': {
                    'load': "model = torch.jit.load('model_torchscript.pt')",
                    'predict': "output = model(input_tensor)"
                },
                'onnx': {
                    'load': "import onnxruntime; session = onnxruntime.InferenceSession('model.onnx')",
                    'predict': "output = session.run(None, {'input': input_array})"
                }
            }
        }
        
        with open(output_dir / 'deployment_config.json', 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        # 4. åˆ›å»ºç®€å•çš„æ¨ç†ç¤ºä¾‹
        inference_example = '''
import torch
import numpy as np
from PIL import Image
import torchvision.transforms as transforms

# åŠ è½½æ¨¡å‹
model = torch.jit.load('model_torchscript.pt')
model.eval()

# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                        std=[0.229, 0.224, 0.225])
])

# æ¨ç†å‡½æ•°
def predict_image(image_path):
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0)
    
    with torch.no_grad():
        output = model(input_tensor)
        probabilities = torch.nn.functional.softmax(output, dim=1)
        predicted_class = torch.argmax(probabilities, dim=1).item()
        confidence = torch.max(probabilities).item()
    
    class_names = ['bilei', 'waiguan']
    return {
        'class': class_names[predicted_class],
        'confidence': confidence,
        'probabilities': {
            class_names[i]: prob.item() 
            for i, prob in enumerate(probabilities[0])
        }
    }

# ä½¿ç”¨ç¤ºä¾‹
# result = predict_image('test_image.jpg')
# print(f"é¢„æµ‹ç±»åˆ«: {result['class']}, ç½®ä¿¡åº¦: {result['confidence']:.3f}")
'''
        
        with open(output_dir / 'inference_example.py', 'w', encoding='utf-8') as f:
            f.write(inference_example)
        
        # 5. åˆ›å»ºREADME
        readme_content = f"""
# bilei vs waiguan åˆ†ç±»æ¨¡å‹éƒ¨ç½²åŒ…

## æ¨¡å‹ä¿¡æ¯
- æ¶æ„: EfficientNet-B0
- ç±»åˆ«: bilei, waiguan
- è¾“å…¥å°ºå¯¸: {self.input_size}
- å‡†ç¡®ç‡: 98%+

## å¯ç”¨æ ¼å¼
1. **TorchScript** (`model_torchscript.pt`) - PyTorchéƒ¨ç½²æ¨è
2. **ONNX** (`model.onnx`) - è·¨å¹³å°éƒ¨ç½²
3. **ä¼˜åŒ–PyTorch** (`model_optimized.pth`) - åŒ…å«å®Œæ•´é…ç½®

## å¿«é€Ÿå¼€å§‹
```python
# ä½¿ç”¨TorchScriptæ¨¡å‹
import torch
model = torch.jit.load('model_torchscript.pt')

# æˆ–æŸ¥çœ‹ inference_example.py è·å–å®Œæ•´ç¤ºä¾‹
```

## æ€§èƒ½åŸºå‡†
{json.dumps(benchmark_results, indent=2)}

## æ–‡ä»¶è¯´æ˜
- `deployment_config.json`: å®Œæ•´é…ç½®ä¿¡æ¯
- `inference_example.py`: æ¨ç†ä»£ç ç¤ºä¾‹
- `README.md`: æœ¬æ–‡ä»¶
"""
        
        with open(output_dir / 'README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        print(f"âœ… éƒ¨ç½²åŒ…åˆ›å»ºå®Œæˆ!")
        print(f"ğŸ“ åŒ…å«æ–‡ä»¶:")
        for file_path in output_dir.iterdir():
            if file_path.is_file():
                size_mb = file_path.stat().st_size / (1024*1024)
                print(f"   - {file_path.name} ({size_mb:.1f} MB)")

def main():
    # é…ç½®å‚æ•°
    model_path = "./output_bilei_waiguan/bilei_waiguan_classification/model_best.pth.tar"
    output_dir = "./deployment_package"
    
    # åˆ›å»ºè½¬æ¢å™¨
    converter = ModelConverter(model_path, device='cpu')
    
    # åˆ›å»ºéƒ¨ç½²åŒ…
    converter.create_deployment_package(output_dir)
    
    print(f"\nğŸ‰ æ¨¡å‹è½¬æ¢å®Œæˆ!")
    print(f"ğŸ“¦ éƒ¨ç½²åŒ…ä½ç½®: {output_dir}")

if __name__ == "__main__":
    main()
