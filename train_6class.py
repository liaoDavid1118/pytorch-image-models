#!/usr/bin/env python3
"""
6åˆ†ç±»å®Œæ•´è®­ç»ƒè„šæœ¬ - è‡ªåŠ¨è°ƒè¯•ç‰ˆæœ¬
bilei, fuban, genbu, mengpi, qianhou, waiguan
"""

import argparse
import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import timm
from timm.loss import LabelSmoothingCrossEntropy
from timm.utils import accuracy, AverageMeter
import logging
import json
from datetime import datetime

def setup_logging(output_dir):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    os.makedirs(output_dir, exist_ok=True)
    log_file = os.path.join(output_dir, f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return log_file

def create_datasets(data_dir, img_size=224, batch_size=32):
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
    # è®­ç»ƒæ•°æ®å¢å¼º - é’ˆå¯¹6åˆ†ç±»ä¼˜åŒ–
    train_transform = transforms.Compose([
        transforms.Resize((img_size + 32, img_size + 32)),  # ç¨å¤§ä¸€äº›ç”¨äºè£å‰ª
        transforms.RandomCrop((img_size, img_size)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.2),  # å¢åŠ å‚ç›´ç¿»è½¬
        transforms.RandomRotation(degrees=20),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
        transforms.RandomGrayscale(p=0.1),  # éšæœºç°åº¦åŒ–
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        transforms.RandomErasing(p=0.1)  # éšæœºæ“¦é™¤
    ])
    
    # éªŒè¯æ•°æ®é¢„å¤„ç†
    val_transform = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = datasets.ImageFolder(
        root=os.path.join(data_dir, 'train'),
        transform=train_transform
    )
    
    val_dataset = datasets.ImageFolder(
        root=os.path.join(data_dir, 'val'),
        transform=val_transform
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=4,
        pin_memory=True,
        drop_last=True  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=4,
        pin_memory=True
    )
    
    return train_loader, val_loader, train_dataset.classes

def train_epoch(model, train_loader, criterion, optimizer, device, scaler, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    losses = AverageMeter()
    top1 = AverageMeter()
    
    start_time = time.time()
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
        
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        with torch.amp.autocast('cuda'):
            output = model(data)
            loss = criterion(output, target)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        # è®¡ç®—å‡†ç¡®ç‡
        acc1 = accuracy(output, target, topk=(1,))[0]
        losses.update(loss.item(), data.size(0))
        top1.update(acc1.item(), data.size(0))
        
        # æ¯20ä¸ªbatchæ‰“å°ä¸€æ¬¡
        if batch_idx % 20 == 0:
            elapsed = time.time() - start_time
            logging.info(f'Epoch: {epoch:3d} [{batch_idx:4d}/{len(train_loader):4d}] '
                        f'Loss: {losses.avg:.4f} Acc: {top1.avg:6.2f}% '
                        f'Time: {elapsed:.1f}s')
    
    return losses.avg, top1.avg

def validate(model, val_loader, criterion, device, classes):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    losses = AverageMeter()
    top1 = AverageMeter()
    
    # ç”¨äºè®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    class_correct = torch.zeros(len(classes))
    class_total = torch.zeros(len(classes))
    
    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
            
            output = model(data)
            loss = criterion(output, target)
            
            # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
            acc1 = accuracy(output, target, topk=(1,))[0]
            losses.update(loss.item(), data.size(0))
            top1.update(acc1.item(), data.size(0))
            
            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
            _, predicted = torch.max(output, 1)
            for i in range(target.size(0)):
                label = target[i].item()
                class_total[label] += 1
                if predicted[i] == target[i]:
                    class_correct[label] += 1
    
    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    logging.info("å„ç±»åˆ«éªŒè¯å‡†ç¡®ç‡:")
    for i, class_name in enumerate(classes):
        if class_total[i] > 0:
            acc = 100 * class_correct[i] / class_total[i]
            logging.info(f"  {class_name}: {acc:.2f}% ({int(class_correct[i])}/{int(class_total[i])})")
    
    logging.info(f'æ€»ä½“éªŒè¯ - Loss: {losses.avg:.4f} Acc: {top1.avg:.2f}%')
    return losses.avg, top1.avg

def save_training_info(output_dir, args, classes, best_acc, training_time):
    """ä¿å­˜è®­ç»ƒä¿¡æ¯"""
    info = {
        'model': args.model,
        'num_classes': len(classes),
        'classes': classes,
        'batch_size': args.batch_size,
        'epochs': args.epochs,
        'learning_rate': args.lr,
        'best_accuracy': best_acc,
        'training_time_hours': training_time / 3600,
        'timestamp': datetime.now().isoformat()
    }
    
    with open(os.path.join(output_dir, 'training_info.json'), 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=2, ensure_ascii=False)

def main():
    # å›ºå®šå‚æ•°é…ç½® - é’ˆå¯¹6åˆ†ç±»ä¼˜åŒ–
    args = argparse.Namespace(
        data_dir='./dataset',
        model='efficientnet_b0',
        batch_size=32,  # RTX 3060 12GB é€‚åˆçš„æ‰¹æ¬¡å¤§å°
        epochs=80,      # å¢åŠ è®­ç»ƒè½®æ•°
        lr=0.001,
        img_size=224,
        output='./output_6class',
        label_smoothing=0.1,
        weight_decay=0.01
    )
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ å¼€å§‹6åˆ†ç±»è®­ç»ƒ")
    print(f"è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # è®¾ç½®æ—¥å¿—
    log_file = setup_logging(args.output)
    logging.info("=" * 60)
    logging.info("6åˆ†ç±»è®­ç»ƒå¼€å§‹")
    logging.info("=" * 60)
    
    # åˆ›å»ºæ•°æ®é›†
    train_loader, val_loader, classes = create_datasets(
        args.data_dir, args.img_size, args.batch_size
    )
    
    logging.info(f"ç±»åˆ«: {classes}")
    logging.info(f"è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
    logging.info(f"éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
    logging.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    logging.info(f"è®­ç»ƒè½®æ•°: {args.epochs}")
    
    # åˆ›å»ºæ¨¡å‹
    try:
        model = timm.create_model(args.model, pretrained=True, num_classes=len(classes))
        logging.info(f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {args.model}")
    except:
        model = timm.create_model(args.model, pretrained=False, num_classes=len(classes))
        logging.info(f"ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹: {args.model}")
    
    model = model.to(device)
    
    # æŸå¤±å‡½æ•° - ä½¿ç”¨æ ‡ç­¾å¹³æ»‘
    criterion = LabelSmoothingCrossEntropy(smoothing=args.label_smoothing)
    
    # ä¼˜åŒ–å™¨ - AdamW with weight decay
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=args.lr, 
        weight_decay=args.weight_decay,
        betas=(0.9, 0.999)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½™å¼¦é€€ç« + çƒ­èº«
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=20, T_mult=2, eta_min=1e-6
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.amp.GradScaler('cuda')
    
    # è®­ç»ƒå¾ªç¯
    best_acc = 0.0
    best_epoch = 0
    training_start_time = time.time()
    
    logging.info("å¼€å§‹è®­ç»ƒ...")
    
    for epoch in range(args.epochs):
        epoch_start_time = time.time()
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device, scaler, epoch
        )
        
        # éªŒè¯
        val_loss, val_acc = validate(model, val_loader, criterion, device, classes)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        is_best = val_acc > best_acc
        if is_best:
            best_acc = val_acc
            best_epoch = epoch
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_acc': best_acc,
                'classes': classes
            }, os.path.join(args.output, 'best_model.pth'))
            
            logging.info(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! å‡†ç¡®ç‡: {best_acc:.2f}%")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % 10 == 0 or is_best:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_acc': best_acc,
                'classes': classes
            }, os.path.join(args.output, f'checkpoint_epoch_{epoch}.pth'))
        
        epoch_time = time.time() - epoch_start_time
        total_time = time.time() - training_start_time
        
        logging.info(f"Epoch {epoch:3d}/{args.epochs}: "
                    f"è®­ç»ƒLoss={train_loss:.4f} è®­ç»ƒAcc={train_acc:.2f}% "
                    f"éªŒè¯Loss={val_loss:.4f} éªŒè¯Acc={val_acc:.2f}% "
                    f"æœ€ä½³Acc={best_acc:.2f}% LR={current_lr:.6f} "
                    f"è€—æ—¶={epoch_time:.1f}s æ€»æ—¶é—´={total_time/60:.1f}min")
        
        # æ—©åœæ£€æŸ¥
        if epoch - best_epoch > 20:
            logging.info(f"æ—©åœ: è¿ç»­20è½®æ— æ”¹å–„ï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}% (Epoch {best_epoch})")
            break
    
    total_training_time = time.time() - training_start_time
    
    logging.info("=" * 60)
    logging.info("è®­ç»ƒå®Œæˆ!")
    logging.info(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}% (Epoch {best_epoch})")
    logging.info(f"æ€»è®­ç»ƒæ—¶é—´: {total_training_time/3600:.2f} å°æ—¶")
    logging.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    logging.info(f"æœ€ä½³æ¨¡å‹: {os.path.join(args.output, 'best_model.pth')}")
    logging.info("=" * 60)
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    save_training_info(args.output, args, classes, best_acc, total_training_time)
    
    return best_acc

if __name__ == '__main__':
    best_accuracy = main()
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")
