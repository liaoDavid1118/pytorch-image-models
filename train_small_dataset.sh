#!/bin/bash
# 200å¼ å›¾ç‰‡å°æ•°æ®é›†è®­ç»ƒè„šæœ¬
# ä½¿ç”¨æ–¹æ³•: bash train_small_dataset.sh /path/to/your/dataset 10

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# æ£€æŸ¥å‚æ•°
if [ $# -lt 2 ]; then
    echo "ä½¿ç”¨æ–¹æ³•: $0 <æ•°æ®é›†è·¯å¾„> <ç±»åˆ«æ•°é‡> [å¯é€‰:æ¨¡å‹åç§°]"
    echo "ç¤ºä¾‹: $0 /path/to/dataset 10 efficientnet_b0"
    exit 1
fi

DATASET_PATH=$1
NUM_CLASSES=$2
MODEL_NAME=${3:-efficientnet_b0}  # é»˜è®¤ä½¿ç”¨efficientnet_b0

echo "ğŸš€ å¼€å§‹è®­ç»ƒå°æ•°æ®é›†æ¨¡å‹"
echo "æ•°æ®é›†è·¯å¾„: $DATASET_PATH"
echo "ç±»åˆ«æ•°é‡: $NUM_CLASSES"
echo "æ¨¡å‹: $MODEL_NAME"
echo "================================"

# æ£€æŸ¥æ•°æ®é›†è·¯å¾„
if [ ! -d "$DATASET_PATH" ]; then
    echo "âŒ é”™è¯¯: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: $DATASET_PATH"
    exit 1
fi

# æ£€æŸ¥æ•°æ®é›†ç»“æ„
if [ ! -d "$DATASET_PATH/train" ]; then
    echo "âŒ é”™è¯¯: ç¼ºå°‘è®­ç»ƒé›†ç›®å½•: $DATASET_PATH/train"
    exit 1
fi

if [ ! -d "$DATASET_PATH/val" ]; then
    echo "âŒ é”™è¯¯: ç¼ºå°‘éªŒè¯é›†ç›®å½•: $DATASET_PATH/val"
    exit 1
fi

# åˆ›å»ºè¾“å‡ºç›®å½•
OUTPUT_DIR="./output_small_dataset_$(date +%Y%m%d_%H%M%S)"
mkdir -p $OUTPUT_DIR

echo "ğŸ“ è¾“å‡ºç›®å½•: $OUTPUT_DIR"

# æ–¹æ¡ˆ1: å•é˜¶æ®µè®­ç»ƒ (æ¨èæ–°æ‰‹)
echo "ğŸ¯ æ–¹æ¡ˆ1: å•é˜¶æ®µç«¯åˆ°ç«¯è®­ç»ƒ"
python train.py $DATASET_PATH \
    --model $MODEL_NAME \
    --pretrained \
    --num-classes $NUM_CLASSES \
    --batch-size 16 \
    --lr 0.001 \
    --epochs 150 \
    --opt adamw \
    --weight-decay 0.01 \
    --sched cosine \
    --warmup-epochs 10 \
    --aa rand-m15-mstd0.5-inc1 \
    --mixup 0.4 \
    --cutmix 1.0 \
    --reprob 0.3 \
    --drop-path 0.1 \
    --amp \
    --model-ema \
    --model-ema-decay 0.9999 \
    --patience 20 \
    --min-lr 1e-6 \
    --output $OUTPUT_DIR \
    --experiment small_dataset_single_stage \
    --log-interval 10 \
    --recovery-interval 5

echo "âœ… å•é˜¶æ®µè®­ç»ƒå®Œæˆ!"

# éªŒè¯æœ€ä½³æ¨¡å‹
echo "ğŸ” éªŒè¯æœ€ä½³æ¨¡å‹..."
python validate.py $DATASET_PATH \
    --model $MODEL_NAME \
    --checkpoint $OUTPUT_DIR/model_best.pth.tar \
    --batch-size 32 \
    --amp \
    --results-file $OUTPUT_DIR/validation_results.csv

echo "ğŸ“Š éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: $OUTPUT_DIR/validation_results.csv"

# å¦‚æœéœ€è¦ä¸¤é˜¶æ®µè®­ç»ƒï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
: '
echo "ğŸ¯ æ–¹æ¡ˆ2: ä¸¤é˜¶æ®µè®­ç»ƒ (é«˜çº§ç”¨æˆ·)"

# é˜¶æ®µ1: ç‰¹å¾æå– (å†»ç»“backbone)
echo "ğŸ“š é˜¶æ®µ1: ç‰¹å¾æå–è®­ç»ƒ..."
python train.py $DATASET_PATH \
    --model $MODEL_NAME \
    --pretrained \
    --num-classes $NUM_CLASSES \
    --batch-size 32 \
    --lr 0.01 \
    --epochs 50 \
    --opt sgd \
    --momentum 0.9 \
    --weight-decay 0.0001 \
    --sched step \
    --decay-epochs 20 \
    --aa rand-m7-mstd0.5 \
    --mixup 0.2 \
    --output ${OUTPUT_DIR}_stage1 \
    --experiment small_dataset_stage1

# é˜¶æ®µ2: ç«¯åˆ°ç«¯å¾®è°ƒ
echo "ğŸ”§ é˜¶æ®µ2: ç«¯åˆ°ç«¯å¾®è°ƒ..."
python train.py $DATASET_PATH \
    --model $MODEL_NAME \
    --resume ${OUTPUT_DIR}_stage1/model_best.pth.tar \
    --num-classes $NUM_CLASSES \
    --batch-size 16 \
    --lr 0.0005 \
    --epochs 100 \
    --opt adamw \
    --weight-decay 0.01 \
    --sched cosine \
    --warmup-epochs 5 \
    --aa rand-m15-mstd0.5-inc1 \
    --mixup 0.4 \
    --cutmix 1.0 \
    --reprob 0.3 \
    --amp \
    --model-ema \
    --patience 15 \
    --output ${OUTPUT_DIR}_stage2 \
    --experiment small_dataset_stage2

echo "âœ… ä¸¤é˜¶æ®µè®­ç»ƒå®Œæˆ!"
'

echo "ğŸ‰ è®­ç»ƒå®Œæˆ! ç»“æœä¿å­˜åœ¨: $OUTPUT_DIR"
echo "ğŸ“ˆ æŸ¥çœ‹è®­ç»ƒæ—¥å¿—: tensorboard --logdir $OUTPUT_DIR"
echo "ğŸ” æœ€ä½³æ¨¡å‹: $OUTPUT_DIR/model_best.pth.tar"
